{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install naptha-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'naptha_sdk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01muuid\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnaptha_sdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Node\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnaptha_sdk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschemas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AgentRunInput, AgentDeployment, OrchestratorDeployment, OrchestratorRunInput\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'naptha_sdk'"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from naptha_sdk.client.node import Node\n",
    "from naptha_sdk.schemas import AgentRunInput, AgentDeployment, OrchestratorDeployment, OrchestratorRunInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-22 13:15:58,282 - naptha_sdk.client.node - INFO - Node URL: http://node1.naptha.ai:7001\n"
     ]
    }
   ],
   "source": [
    "node = Node(node_url=\"http://node1.naptha.ai:7001\")\n",
    "\n",
    "user_id = str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'public_key': '8066e3ca-f49a-409a-b821-a04501c24f83', 'id': 'user:8066e3ca-f49a-409a-b821-a04501c24f83'}\n"
     ]
    }
   ],
   "source": [
    "user = await node.register_user({\"public_key\": user_id})\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user:8066e3ca-f49a-409a-b821-a04501c24f83'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id = user[\"id\"]\n",
    "user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running single agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_run_input = AgentRunInput(\n",
    "    consumer_id=user_id,\n",
    "    name=\"hello_world_agent\",\n",
    "    agent_deployment=AgentDeployment(\n",
    "        name=\"hello_world_agent\",\n",
    "        module={\n",
    "            \"name\": \"hello_world_agent\",\n",
    "        },\n",
    "    ),\n",
    "    inputs={\n",
    "        \"firstname\": \"John\",\n",
    "        \"surname\": \"Doe\",\n",
    "    }\n",
    ")\n",
    "\n",
    "result = await node.run_agent_and_poll(agent_run_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running multiagent_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run input: consumer_id='user:8066e3ca-f49a-409a-b821-a04501c24f83' inputs={'prompt': 'Count to 20, one at a time. I will start. one'} orchestrator_deployment=OrchestratorDeployment(name='multiagent_chat', module={'name': 'multiagent_chat'}, orchestrator_node_url='http://localhost:7001', orchestrator_config=OrchestratorConfig(config_name='orchestrator_config', max_rounds=5), environment_deployments=[EnvironmentDeployment(name='environment_deployment', module=None, environment_node_url='http://localhost:7001', environment_config=EnvironmentConfig(config_name='environment_config', environment_type=None))], agent_deployments=[AgentDeployment(name='agent_deployment', module=None, worker_node_url='ws://localhost:7002', agent_config=AgentConfig(config_name='agent_config', llm_config=LLMConfig(config_name='llm_config', client=None, model=None, max_tokens=None, temperature=None, api_base=None), persona_module=None, system_prompt=None), data_generation_config=DataGenerationConfig(save_outputs=None, save_outputs_location=None, save_outputs_path=None, save_inputs=None, save_inputs_location=None), kb_deployments=None), AgentDeployment(name='agent_deployment', module=None, worker_node_url='ws://localhost:7002', agent_config=AgentConfig(config_name='agent_config', llm_config=LLMConfig(config_name='llm_config', client=None, model=None, max_tokens=None, temperature=None, api_base=None), persona_module=None, system_prompt=None), data_generation_config=DataGenerationConfig(save_outputs=None, save_outputs_location=None, save_outputs_path=None, save_inputs=None, save_inputs_location=None), kb_deployments=None)])\n",
      "Module type: orchestrator\n",
      "Running orchestrator...\n",
      "Run input: consumer_id='user:8066e3ca-f49a-409a-b821-a04501c24f83' inputs={'prompt': 'Count to 20, one at a time. I will start. one'} orchestrator_deployment=OrchestratorDeployment(name='multiagent_chat', module={'name': 'multiagent_chat'}, orchestrator_node_url='http://localhost:7001', orchestrator_config=OrchestratorConfig(config_name='orchestrator_config', max_rounds=5), environment_deployments=[EnvironmentDeployment(name='environment_deployment', module=None, environment_node_url='http://localhost:7001', environment_config=EnvironmentConfig(config_name='environment_config', environment_type=None))], agent_deployments=[AgentDeployment(name='agent_deployment', module=None, worker_node_url='ws://localhost:7002', agent_config=AgentConfig(config_name='agent_config', llm_config=LLMConfig(config_name='llm_config', client=None, model=None, max_tokens=None, temperature=None, api_base=None), persona_module=None, system_prompt=None), data_generation_config=DataGenerationConfig(save_outputs=None, save_outputs_location=None, save_outputs_path=None, save_inputs=None, save_inputs_location=None), kb_deployments=None), AgentDeployment(name='agent_deployment', module=None, worker_node_url='ws://localhost:7002', agent_config=AgentConfig(config_name='agent_config', llm_config=LLMConfig(config_name='llm_config', client=None, model=None, max_tokens=None, temperature=None, api_base=None), persona_module=None, system_prompt=None), data_generation_config=DataGenerationConfig(save_outputs=None, save_outputs_location=None, save_outputs_path=None, save_inputs=None, save_inputs_location=None), kb_deployments=None)])\n",
      "Node URL: http://node1.naptha.ai:7001\n",
      "Orchestrator run started: consumer_id='user:8066e3ca-f49a-409a-b821-a04501c24f83' inputs={'prompt': 'Count to 20, one at a time. I will start. one'} orchestrator_deployment=OrchestratorDeployment(name='multiagent_chat', module={'id': 'orchestrator:multiagent_chat', 'name': 'multiagent_chat', 'description': 'Multi-agent network for chat', 'author': 'user:naptha', 'module_url': 'https://github.com/NapthaAI/multiagent_chat', 'module_type': 'package', 'module_version': '0.14', 'module_entrypoint': 'run.py', 'personas_urls': None}, orchestrator_node_url='http://localhost:7001', orchestrator_config=OrchestratorConfig(config_name='orchestrator_config', max_rounds=5), environment_deployments=[EnvironmentDeployment(name='environment_deployment', module=None, environment_node_url='http://localhost:7001', environment_config=EnvironmentConfig(config_name='environment_config', environment_type=None))], agent_deployments=[AgentDeployment(name='agent_deployment', module=None, worker_node_url='ws://localhost:7002', agent_config=AgentConfig(config_name='agent_config', llm_config=LLMConfig(config_name='llm_config', client=None, model=None, max_tokens=None, temperature=None, api_base=None), persona_module=None, system_prompt=None), data_generation_config=DataGenerationConfig(save_outputs=None, save_outputs_location=None, save_outputs_path=None, save_inputs=None, save_inputs_location=None), kb_deployments=None), AgentDeployment(name='agent_deployment', module=None, worker_node_url='ws://localhost:7002', agent_config=AgentConfig(config_name='agent_config', llm_config=LLMConfig(config_name='llm_config', client=None, model=None, max_tokens=None, temperature=None, api_base=None), persona_module=None, system_prompt=None), data_generation_config=DataGenerationConfig(save_outputs=None, save_outputs_location=None, save_outputs_path=None, save_inputs=None, save_inputs_location=None), kb_deployments=None)]) status='pending' error=False id='a60e8bf8-22eb-494e-b185-44447f8589fd' results=[] error_message=None created_time=None start_processing_time=None completed_time=None duration=None agent_runs=[] input_schema_ipfs_hash=None\n",
      "running package multiagent_chat\n",
      "running package multiagent_chat\n",
      "running package multiagent_chat\n",
      "running package multiagent_chat\n",
      "running package multiagent_chat\n",
      "running package multiagent_chat\n",
      "running package multiagent_chat\n",
      "completed package multiagent_chat\n",
      "Output:  [{\"role\": \"system\", \"content\": \"{\\\"role\\\": \\\"You are a helpful AI assistant.\\\", \\\"persona\\\": \\\"\\\"}\"}, {\"role\": \"user\", \"content\": \"Count to 20, one at a time. I will start. one\"}, {\"role\": \"assistant\", \"content\": \"Two.\"}, {\"role\": \"user\", \"content\": \"Three.\"}, {\"role\": \"assistant\", \"content\": \"Four.\"}, {\"role\": \"user\", \"content\": \"Five.\"}, {\"role\": \"assistant\", \"content\": \"Six.\"}, {\"role\": \"user\", \"content\": \"Seven.\"}, {\"role\": \"assistant\", \"content\": \"Eight.\"}, {\"role\": \"user\", \"content\": \"Nine.\"}, {\"role\": \"assistant\", \"content\": \"Ten.\"}, {\"role\": \"user\", \"content\": \"Eleven.\"}, {\"role\": \"assistant\", \"content\": \"Twelve.\"}, {\"role\": \"user\", \"content\": \"Thirteen.\"}, {\"role\": \"assistant\", \"content\": \"Fourteen.\"}, {\"role\": \"user\", \"content\": \"Fifteen.\"}, {\"role\": \"assistant\", \"content\": \"Sixteen.\"}, {\"role\": \"user\", \"content\": \"Seventeen.\"}, {\"role\": \"assistant\", \"content\": \"Eighteen.\"}, {\"role\": \"user\", \"content\": \"Nineteen.\"}, {\"role\": \"assistant\", \"content\": \"Twenty.\"}, {\"role\": \"user\", \"content\": \"Great job! We counted to twenty together!\"}]\n",
      "['[{\"role\": \"system\", \"content\": \"{\\\\\"role\\\\\": \\\\\"You are a helpful AI assistant.\\\\\", \\\\\"persona\\\\\": \\\\\"\\\\\"}\"}, {\"role\": \"user\", \"content\": \"Count to 20, one at a time. I will start. one\"}, {\"role\": \"assistant\", \"content\": \"Two.\"}, {\"role\": \"user\", \"content\": \"Three.\"}, {\"role\": \"assistant\", \"content\": \"Four.\"}, {\"role\": \"user\", \"content\": \"Five.\"}, {\"role\": \"assistant\", \"content\": \"Six.\"}, {\"role\": \"user\", \"content\": \"Seven.\"}, {\"role\": \"assistant\", \"content\": \"Eight.\"}, {\"role\": \"user\", \"content\": \"Nine.\"}, {\"role\": \"assistant\", \"content\": \"Ten.\"}, {\"role\": \"user\", \"content\": \"Eleven.\"}, {\"role\": \"assistant\", \"content\": \"Twelve.\"}, {\"role\": \"user\", \"content\": \"Thirteen.\"}, {\"role\": \"assistant\", \"content\": \"Fourteen.\"}, {\"role\": \"user\", \"content\": \"Fifteen.\"}, {\"role\": \"assistant\", \"content\": \"Sixteen.\"}, {\"role\": \"user\", \"content\": \"Seventeen.\"}, {\"role\": \"assistant\", \"content\": \"Eighteen.\"}, {\"role\": \"user\", \"content\": \"Nineteen.\"}, {\"role\": \"assistant\", \"content\": \"Twenty.\"}, {\"role\": \"user\", \"content\": \"Great job! We counted to twenty together!\"}]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrchestratorRun(consumer_id='user:8066e3ca-f49a-409a-b821-a04501c24f83', inputs={}, orchestrator_deployment=OrchestratorDeployment(name='multiagent_chat', module={'id': 'orchestrator:multiagent_chat', 'name': 'multiagent_chat', 'description': 'Multi-agent network for chat', 'author': 'user:naptha', 'module_url': 'https://github.com/NapthaAI/multiagent_chat', 'module_type': 'package', 'module_version': '0.14', 'module_entrypoint': 'run.py', 'personas_urls': None}, orchestrator_node_url='http://localhost:7001', orchestrator_config=OrchestratorConfig(config_name='orchestrator_config', max_rounds=5), environment_deployments=[EnvironmentDeployment(name='environment_deployment', module=None, environment_node_url='http://localhost:7001', environment_config=EnvironmentConfig(config_name='environment_config', environment_type=None))], agent_deployments=[AgentDeployment(name='agent_deployment_1', module={'name': 'simple_chat_agent'}, worker_node_url='ws://localhost:7002', agent_config=AgentConfig(config_name='agent_config_1', llm_config=LLMConfig(config_name='model_2', client=<LLMClientType.OPENAI: 'openai'>, model='gpt-4o-mini', max_tokens=1000, temperature=0.7, api_base='https://api.openai.com/v1'), persona_module={'module_url': 'https://huggingface.co/datasets/richardblythman/characterfile_richardblythman'}, system_prompt={'role': 'You are a helpful AI assistant.', 'persona': ''}), data_generation_config=DataGenerationConfig(save_outputs=None, save_outputs_location=None, save_outputs_path=None, save_inputs=None, save_inputs_location=None), kb_deployments=None), AgentDeployment(name='agent_deployment_2', module={'name': 'simple_chat_agent'}, worker_node_url='ws://localhost:7002', agent_config=AgentConfig(config_name='agent_config_2', llm_config=LLMConfig(config_name='model_2', client=<LLMClientType.OPENAI: 'openai'>, model='gpt-4o-mini', max_tokens=1000, temperature=0.7, api_base='https://api.openai.com/v1'), persona_module={'module_url': 'https://huggingface.co/datasets/richardblythman/characterfile_richardblythman'}, system_prompt={'role': 'You are a helpful AI assistant.', 'persona': ''}), data_generation_config=DataGenerationConfig(save_outputs=None, save_outputs_location=None, save_outputs_path=None, save_inputs=None, save_inputs_location=None), kb_deployments=None)]), status='completed', error=False, id='a60e8bf8-22eb-494e-b185-44447f8589fd', results=['[{\"role\": \"system\", \"content\": \"{\\\\\"role\\\\\": \\\\\"You are a helpful AI assistant.\\\\\", \\\\\"persona\\\\\": \\\\\"\\\\\"}\"}, {\"role\": \"user\", \"content\": \"Count to 20, one at a time. I will start. one\"}, {\"role\": \"assistant\", \"content\": \"Two.\"}, {\"role\": \"user\", \"content\": \"Three.\"}, {\"role\": \"assistant\", \"content\": \"Four.\"}, {\"role\": \"user\", \"content\": \"Five.\"}, {\"role\": \"assistant\", \"content\": \"Six.\"}, {\"role\": \"user\", \"content\": \"Seven.\"}, {\"role\": \"assistant\", \"content\": \"Eight.\"}, {\"role\": \"user\", \"content\": \"Nine.\"}, {\"role\": \"assistant\", \"content\": \"Ten.\"}, {\"role\": \"user\", \"content\": \"Eleven.\"}, {\"role\": \"assistant\", \"content\": \"Twelve.\"}, {\"role\": \"user\", \"content\": \"Thirteen.\"}, {\"role\": \"assistant\", \"content\": \"Fourteen.\"}, {\"role\": \"user\", \"content\": \"Fifteen.\"}, {\"role\": \"assistant\", \"content\": \"Sixteen.\"}, {\"role\": \"user\", \"content\": \"Seventeen.\"}, {\"role\": \"assistant\", \"content\": \"Eighteen.\"}, {\"role\": \"user\", \"content\": \"Nineteen.\"}, {\"role\": \"assistant\", \"content\": \"Twenty.\"}, {\"role\": \"user\", \"content\": \"Great job! We counted to twenty together!\"}]'], error_message='', created_time=None, start_processing_time='2024-12-22T05:16:03.596987', completed_time='2024-12-22T05:16:27.236324', duration=24.0, agent_runs=[], input_schema_ipfs_hash=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orchestrator_run_input = OrchestratorRunInput(\n",
    "    consumer_id=user_id,\n",
    "    orchestrator_deployment=OrchestratorDeployment(\n",
    "        name=\"multiagent_chat\",\n",
    "        module={\n",
    "            \"name\": \"multiagent_chat\",\n",
    "        },\n",
    "        agent_deployments=[{\n",
    "            \"worker_node_url\": \"ws://localhost:7002\",\n",
    "        },\n",
    "        {\n",
    "            \"worker_node_url\": \"ws://localhost:7002\",\n",
    "        }\n",
    "        ],\n",
    "        environment_deployments=[{\n",
    "            \"environment_node_url\": \"http://localhost:7001\",\n",
    "        }],\n",
    "    ),\n",
    "    inputs={\n",
    "        \"prompt\": \"Count to 20, one at a time. I will start. one\"\n",
    "    }\n",
    ")\n",
    "\n",
    "result = await node.run_orchestrator_and_poll(orchestrator_run_input)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
